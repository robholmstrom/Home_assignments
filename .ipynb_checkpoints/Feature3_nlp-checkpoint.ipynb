{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/robholmstrom/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/robholmstrom/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/robholmstrom/miniconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/Users/robholmstrom/miniconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk import pos_tag\n",
    "from nltk.text import Text\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "!python -m spacy download en\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# the chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the cleaned novels. this can take a bit\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# combine the sentences from the two novels into one data frame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid off stop words and punctuation\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to vectorize our words using word2vec. For this purpuse, we use Word2Vec from the models module of gensim. The Word2Vec class has several parameters. We set the following parameters:\n",
    "\n",
    "workers=4: We set the number of threads to run in parallel to 4 (make sense if your computer has available computing units).\n",
    "min_count=1: We set the minimum word count threshold to 1.\n",
    "window=6: We set the number of words around target word to consider to 6.\n",
    "sg=0: We use CBOW because our corpus is small.\n",
    "sample=1e-3: We penalize frequent words.\n",
    "size=100: We set the word vector length to 100.\n",
    "hs=1: We use hierarchical softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec on the the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=2,\n",
    "    window=4,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>0.223172</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>-0.055817</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>-0.032733</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.031529</td>\n",
       "      <td>-0.004377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009629</td>\n",
       "      <td>0.114793</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.064799</td>\n",
       "      <td>-0.024737</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>-0.042820</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>0.183989</td>\n",
       "      <td>-0.060073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>0.194484</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>-0.050899</td>\n",
       "      <td>-0.007056</td>\n",
       "      <td>-0.026646</td>\n",
       "      <td>-0.002122</td>\n",
       "      <td>0.030310</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022690</td>\n",
       "      <td>0.092844</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>0.044662</td>\n",
       "      <td>-0.045043</td>\n",
       "      <td>0.036374</td>\n",
       "      <td>-0.037489</td>\n",
       "      <td>-0.055718</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>-0.062522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>remarkable Alice think way hear Rabbit</td>\n",
       "      <td>0.175725</td>\n",
       "      <td>0.032291</td>\n",
       "      <td>-0.015428</td>\n",
       "      <td>0.027251</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.058039</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>0.014530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033440</td>\n",
       "      <td>0.081630</td>\n",
       "      <td>0.138925</td>\n",
       "      <td>0.018392</td>\n",
       "      <td>-0.054507</td>\n",
       "      <td>0.068657</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>-0.045345</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>-0.028706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>0.153330</td>\n",
       "      <td>0.070060</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.033633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>0.046761</td>\n",
       "      <td>-0.023423</td>\n",
       "      <td>-0.059364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.088784</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>-0.040165</td>\n",
       "      <td>0.026241</td>\n",
       "      <td>0.030117</td>\n",
       "      <td>-0.077189</td>\n",
       "      <td>0.091077</td>\n",
       "      <td>-0.099297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>0.153330</td>\n",
       "      <td>0.070060</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.033633</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>0.046761</td>\n",
       "      <td>-0.023423</td>\n",
       "      <td>-0.059364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.088784</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>-0.040165</td>\n",
       "      <td>0.026241</td>\n",
       "      <td>0.030117</td>\n",
       "      <td>-0.077189</td>\n",
       "      <td>0.091077</td>\n",
       "      <td>-0.099297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  Alice begin tired sit sister bank have twice p...  0.223172   \n",
       "1  Carroll  consider mind hot day feel sleepy stupid pleas...  0.194484   \n",
       "2  Carroll             remarkable Alice think way hear Rabbit  0.175725   \n",
       "3  Carroll                                            oh dear  0.153330   \n",
       "4  Carroll                                            oh dear  0.153330   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.020525 -0.055817  0.017764 -0.032733  0.013045  0.031529 -0.004377  ...   \n",
       "1  0.032125 -0.050899 -0.007056 -0.026646 -0.002122  0.030310  0.001089  ...   \n",
       "2  0.032291 -0.015428  0.027251  0.001575  0.058039 -0.001594  0.014530  ...   \n",
       "3  0.070060 -0.007479 -0.033633  0.106510  0.046761 -0.023423 -0.059364  ...   \n",
       "4  0.070060 -0.007479 -0.033633  0.106510  0.046761 -0.023423 -0.059364  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.009629  0.114793  0.145700  0.064799 -0.024737  0.013459 -0.042820   \n",
       "1 -0.022690  0.092844  0.132692  0.044662 -0.045043  0.036374 -0.037489   \n",
       "2 -0.033440  0.081630  0.138925  0.018392 -0.054507  0.068657  0.027283   \n",
       "3  0.007347  0.109599  0.088784  0.049615 -0.040165  0.026241  0.030117   \n",
       "4  0.007347  0.109599  0.088784  0.049615 -0.040165  0.026241  0.030117   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.070483  0.183989 -0.060073  \n",
       "1 -0.055718  0.147318 -0.062522  \n",
       "2 -0.045345  0.131372 -0.028706  \n",
       "3 -0.077189  0.091077 -0.099297  \n",
       "4 -0.077189  0.091077 -0.099297  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    try:\n",
    "        word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "    except KeyError:\n",
    "        word2vec_arr[i, :] = np.full((1,100), np.nan)\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences2 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences2.dropna(inplace=True)\n",
    "\n",
    "sentences2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your own word2vec representations as we did in our first example in the checkpoint. But, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.6875\n",
      "\n",
      "Test set score: 0.6798438262567106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences2['author']\n",
    "X = np.array(sentences2.drop(['text','author'], 1))\n",
    "\n",
    "# We split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
